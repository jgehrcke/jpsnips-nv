import logging
import uuid
import time
import sys
import os
import signal
import threading
import pickle

from typing import NewType, cast
from datetime import datetime
from http.server import ThreadingHTTPServer, BaseHTTPRequestHandler
from pprint import pformat

import requests

# Note(JP): nccl provides excellent log output, revealing much of what it's
# doing under the hood (great for understanding), but also seemingly pretty good
# error messages.
# Show info levels, interleaved with stdout/stderr.
# Enable timestamps for INFO msgs and higher.
# os.environ["NCCL_DEBUG"] = "INFO"
os.environ["NCCL_DEBUG"] = "INFO"

# os.environ["NCCL_DEBUG"] = "WARN"
os.environ["NCCL_DEBUG_TIMESTAMP_LEVELS"] = "WARN,INFO,TRACE"
os.environ["NCCL_DEBUG_TIMESTAMP_FORMAT"] = "[%F %T.%6f]"

import cupy
from cupy.cuda import nccl
import numpy

log = logging.getLogger()
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s.%(msecs)03dZ %(levelname)s: %(message)s",
    datefmt="%Y-%m-%dT%H:%M:%S",
)

# Explicit UTC timezone.
logging.Formatter.converter = time.gmtime

# global dictionary, thread-safe updates
DYNCFG = {"nccl_comm_id": "not-set"}

# 10000 is a decent edge length to start with 35000x35000 seems to translate to
# ~5 GB in GPU memory. Going up and down from there with a simple scale factor
# that can be set via environment variable.
LARGE_PAYLOAD_SHAPE = (int(4.5 * 10**4), int(4.5 * 10**4))
if "NICKELPIE_MATRIX_SCALE" in os.environ:
    _scalef = float(os.environ["NICKELPIE_MATRIX_SCALE"])
    LARGE_PAYLOAD_SHAPE = (int(_scalef * 10**4), int(_scalef * 10**4))


LARGE_PAYLOAD_DTYPE = numpy.float32  # 64
LARGE_PAYLOAD_DTYPE_NCCL = nccl.NCCL_FLOAT32  # 64
LARGE_PAYLOAD_BYTES_PER_VALUE = 4
LARGE_PAYLOAD_VALUE_COUNT = LARGE_PAYLOAD_SHAPE[0] * LARGE_PAYLOAD_SHAPE[1]
LARGE_PAYLOAD_SIZE_MB = (LARGE_PAYLOAD_VALUE_COUNT * LARGE_PAYLOAD_BYTES_PER_VALUE) / (
    10**6
)

# At an expected bandwidth of about 700 GB/s, pick an amount that will take a
# small number of seconds to be transmitted.
SEND_TOTAL_GB_ACROSS_REPETITIONS = 10000

if "NICKELPIE_SEND_TOTAL_GB_PER_BENCHMARK" in os.environ:
    SEND_TOTAL_GB_ACROSS_REPETITIONS = int(
        os.environ["NICKELPIE_SEND_TOTAL_GB_PER_BENCHMARK"]
    )

SENDRECV_LOOP_REPETITIONS = int(
    SEND_TOTAL_GB_ACROSS_REPETITIONS / (LARGE_PAYLOAD_SIZE_MB / 1000.0)
)


# Prepare to store events for synchronization (across threads).
EVENTS = {}


LEADER_HTTPD_BASE_URL = (
    "http://"
    + os.environ["NICKELPIE_HTTPD_DNSNAME"]
    + ":"
    + os.environ["NICKELPIE_HTTPD_PORT"]
)


# Pragmatic distinct type (because cupy isn't typed)
MatrixBuf = NewType("MatrixBuf", object)


def main():
    log_debug_info_assert_env()

    n_ranks = int(os.environ["NICKELPIE_RANKS"])
    jci = int(os.environ["JOB_COMPLETION_INDEX"])
    log.info("k8s job completion index: %s", jci)

    if jci == 0:
        leader(n_ranks)

    else:
        follower(jci, n_ranks)

    # Not yet sure if I tear down NCCL properly, wait for more logs to appear.
    # for _ in range(1200):
    #     time.sleep(30)
    #     log.info("wait")

    saw = os.environ.get("NICKELPIE_SLEEP_AFTER_WORK")
    if saw:
        # Not None, not empty string
        log.info('NICKELPIE_SLEEP_AFTER_WORK: %s', saw)
        time.sleep(float(saw))

    log.info("shutdown")


def leader(n_ranks: int):
    # From NCCL docs: "Before calling ncclCommInitRank(), you need to first
    # create a unique object which will be used by all processes and threads to
    # synchronize and understand they are part of the same communicator. This is
    # done by calling the ncclGetUniqueId() function. The ncclGetUniqueId()
    # function returns an ID which has to be broadcast to all participating
    # threads and processes using any CPU communication system, for example,
    # passing the ID pointer to multiple threads, or broadcasting it to other
    # processes using MPI or another parallel environment using, for example,
    # sockets."
    comms_id = nccl.get_unique_id()

    log.info("generated NCCL commnication ID: %s", comms_id)

    # The communication ID is what needs to be generated once and then shared
    # with all communicating peers. This value is generated by the NCCL C
    # library obviously and in Python this is exposed as a tuple. That's cool.
    # Just note that this ID is not a string.
    log.info("comms ID type: %s", type(comms_id))

    # Make available process-globally.
    DYNCFG["nccl_comm_id"] = comms_id

    # Start the HTTP server for exposing that ID to consumers (start server
    # after ID generation: order is important: don't expose the 'not-set' value
    # to consumers). Note that this is an implicit barrier for followers: each
    # follower periodically requests the communication ID and only moves on once
    # the leader's HTTP server has responded.
    run_httpd_in_thread()

    # As leader, we make rank 0. Do not explicitly acquire specific CUDA device,
    # operations below will automatically act on the single device available to
    # the process. (At some point, I used the context manager `with
    # cupy.cuda.Device(0) as _:` here but that is not necessary.
    c = create_nccl_communicator(n_ranks, comms_id, 0)

    # Specific seed for reproducibility.
    cupy.random.set_random_state(
        cupy.random.RandomState(
            seed=1, method=cupy.cuda.curand.CURAND_RNG_PSEUDO_DEFAULT  # type: ignore
        )
    )

    # Prepare payload to send (on GPU).
    with Timer("generate payload matrix"):
        payload, _ = generate_rnd_matrix()

    # All ranks excluding self (0).
    follower_ranks = list(range(1, n_ranks))

    # Send payload to each follower, sequentially.
    with Timer("send-matrix-sequentially-to-each-follower"):
        for r in range(1, n_ranks):
            send_matrix(c, r, payload)

    # All followers except for the "last served" are already waiting for this
    # signal (waiting for us to tell them: ok, broadcast benchmark!). Send
    # payload to all followers, in a single NCCL-provided broadcast operation.
    sync_with_follower_on_barrier("COLLECTIVE_BROADCAST_BENCHMARK")
    broadcast_matrix(c, payload, follower_ranks)

    # It seems to be important to not destroy the NCCL communicator in a
    # follower before the collective work is done. This proceeds in the leader
    # as soon as the first follower reaches this barrier (doesn't wait
    # explicitly until _each_ follower reached the barrier, can improve later,
    # is probably not a problem in practice because all followers are pretty
    # much synced up here because they left the broadcast at the "same time").
    sync_with_follower_on_barrier("COLLECTIVE_SHUTDOWN")

    tear_down_communicator(c)
    log.info("done: sending")


def follower(jci, n_ranks: int):
    """
    jci: Job completion index, injected by k8s. Is at least 1. We're a follower
    (leader is 0). This translates to 'rank'.
    """
    nccl_communication_id = wait_for_comm_id_from_leader()

    # Assume we run in a container that gets precisely one GPU injected.
    c = create_nccl_communicator(
        n_ranks=n_ranks, comms_id=nccl_communication_id, rank=jci
    )

    # All ranks excluding 0.
    follower_ranks = list(range(1, n_ranks))
    recvbuf = follower_prepare_gpu_buf()

    # Wait for the on-GPU NCCL communicator logic to have completed.
    # Calling send() or recv() before that might result in a hang.
    # I tried doing that with a CUDA event, but that didn't help.
    # log.info("wait for NCCL communicator setup")
    # time.sleep(3)
    recv_matrix(c, jci, recvbuf)

    sync_with_leader_on_barrier("COLLECTIVE_BROADCAST_BENCHMARK")
    broadcast_matrix(c, recvbuf, follower_ranks)

    sync_with_leader_on_barrier("COLLECTIVE_SHUTDOWN")

    tear_down_communicator(c)


def follower_prepare_gpu_buf() -> MatrixBuf:
    # recvbuf = cupy.zeros(10, dtype=cupy.int64) Note(JP): tried working with a
    # sparse matrix here but could not get the pointer magic to work -- it's
    # pointless, since the size of the data is known -- just allocate empty
    # (should be fast, right?). recvbuf =
    # cupyx.scipy.sparse.coo_matrix(LARGE_PAYLOAD_SHAPE,
    # dtype=LARGE_PAYLOAD_DTYPE)

    # Documented with "Returns an array without initializing the
    # elements"
    t0 = time.monotonic()
    recvbuf = cast(
        MatrixBuf, cupy.empty(LARGE_PAYLOAD_SHAPE, dtype=LARGE_PAYLOAD_DTYPE)
    )
    log.info("empty() for recv took %.3f s", time.monotonic() - t0)
    return recvbuf


def recv_matrix(communicator, jci: int, recvbuf: MatrixBuf):
    log.info("recv_matrix: warmup")

    sync_with_leader_on_barrier("WARMUP_EXCHANGE_" + str(jci))

    # Receive warmup matrix.
    ev1 = cupy.cuda.Event()
    ev2 = cupy.cuda.Event()
    with Timer(f"communicator.recv() warumup"):
        ev1.record()
        communicator.recv(
            recvbuf.data.ptr,
            LARGE_PAYLOAD_VALUE_COUNT,
            LARGE_PAYLOAD_DTYPE_NCCL,
            0,
            cupy.cuda.Stream.null.ptr,
        )
        ev2.record()

    log.info("synchronize with GPU: wait for last recv() to have completed")
    ev2.synchronize()
    log.info(
        "warmup: recv() blocked GPU for %.5f s",
        cupy.cuda.get_elapsed_time(ev1, ev2) / 1000.0,
    )

    # Before entering the timed recv() loop make sure that the leader enters its
    # send() loop at pretty much the same time.
    sync_with_leader_on_barrier("BENCHMARK_" + str(jci))

    log.info("recv() loop: enter")
    ev1 = cupy.cuda.Event()
    ev2 = cupy.cuda.Event()
    ev1.record()
    with Timer(f"recv()-from-0: {SENDRECV_LOOP_REPETITIONS} reps"):
        for i in range(1, SENDRECV_LOOP_REPETITIONS + 1):
            communicator.recv(
                recvbuf.data.ptr,
                LARGE_PAYLOAD_VALUE_COUNT,
                LARGE_PAYLOAD_DTYPE_NCCL,
                0,
                cupy.cuda.Stream.null.ptr,
            )
        # For now, just receive and don't look at the data. Looking is required
        # for checksum verification. But that takes time and distracts from
        # bandwidth measurement.
        # log.info("recvbuf checksum: %s", cupy.sum(recvbuf))
    ev2.record()
    log.info("recv() loop: done (all calls emitted, maybe not yet done on GPU)")

    # Block CPU execution until event is recorded (done) on GPU, i.e. receiving
    # is really done.
    log.info("synchronize with GPU: wait for last recv() to have completed")
    ev2.synchronize()
    elapsed_s = cupy.cuda.get_elapsed_time(ev1, ev2) / 1000.0
    log.info("recv() loop took overall on GPU: %.3f s", elapsed_s)
    log_transfer_stats(elapsed_s, f"send-recv-0->{jci}({jci})")


def send_matrix(communicator, recv_rank: int, data: MatrixBuf):
    """
    Work in the context of an acquired GPU device.
    """
    log.info("prepare: send matrix to rank %s", recv_rank)

    sync_with_follower_on_barrier("WARMUP_EXCHANGE_" + str(recv_rank))

    # A complete warmup send()/recv() for CUDA, NCCL.

    # Call send(). This is blocking on the GPU (stream) until a corresponding
    # recv() call is made. Use CUDA events to measure exactly how long this
    # blocks the stream (this likely executes on the default stream (0)).
    ev1 = cupy.cuda.Event()
    ev2 = cupy.cuda.Event()
    with Timer(f"communicator.send() to rank {recv_rank}: warumup"):
        ev1.record()
        communicator.send(
            data.data.ptr,
            LARGE_PAYLOAD_VALUE_COUNT,
            LARGE_PAYLOAD_DTYPE_NCCL,
            recv_rank,
            cupy.cuda.Stream.null.ptr,
        )
        ev2.record()

    log.info("synchronize with GPU: wait for last send() to have completed")
    ev2.synchronize()
    log.info(
        "warmup: send() blocked GPU for %.5f s",
        cupy.cuda.get_elapsed_time(ev1, ev2) / 1000.0,
    )

    sync_with_follower_on_barrier("BENCHMARK_" + str(recv_rank))

    ev1 = cupy.cuda.Event()
    ev2 = cupy.cuda.Event()

    with Timer(f"send()-to-{recv_rank}: {SENDRECV_LOOP_REPETITIONS} reps"):
        ev1.record()
        for _ in range(1, SENDRECV_LOOP_REPETITIONS + 1):
            communicator.send(
                data.data.ptr,
                LARGE_PAYLOAD_VALUE_COUNT,
                LARGE_PAYLOAD_DTYPE_NCCL,
                recv_rank,
                cupy.cuda.Stream.null.ptr,
            )
        ev2.record()

    log.info("send() loop: done (all calls emitted, maybe not yet done on GPU)")

    # Block CPU execution until event is recorded (done) on GPU, i.e.
    # sending is really done.
    log.info("synchronize with GPU: wait for last send() to have completed")
    ev2.synchronize()
    elapsed_s = cupy.cuda.get_elapsed_time(ev1, ev2) / 1000.0
    log.info("send()-to-%s loop took overall on GPU: %.3f s", recv_rank, elapsed_s)
    log_transfer_stats(elapsed_s, f"send-recv-0->{recv_rank}(0)")


def broadcast_matrix(communicator, payload: MatrixBuf, follower_ranks: list[int]):
    """
    Same code called on root rank as receiving ranks.
    """
    myrank = communicator.rank_id()

    log.info(
        "prepare: broadcast matrix 0->ranks(%s) (current rank: %s)",
        follower_ranks,
        myrank,
    )

    # Assume that follower processes are synced up already and jump right into
    # broadcast().

    ev1 = cupy.cuda.Event()
    ev2 = cupy.cuda.Event()

    with Timer(f"communicator.broadcast(): {SENDRECV_LOOP_REPETITIONS} reps"):
        ev1.record()
        for _ in range(1, SENDRECV_LOOP_REPETITIONS + 1):
            communicator.broadcast(
                # sendbuff is only used on rank root and ignored for other ranks.
                # That is, the sender is where `communicator.rank_id == 0`
                payload.data.ptr,  # sendbuff
                # In-place operation will happen if sendbuff == recvbuff.
                payload.data.ptr,
                LARGE_PAYLOAD_VALUE_COUNT,
                LARGE_PAYLOAD_DTYPE_NCCL,
                # Root (the sender: in this case we for now make this: 0, the
                # leader).
                0,
                # Default stream (I guess)
                cupy.cuda.Stream.null.ptr,
            )
        ev2.record()

    # Block CPU execution until event is recorded (done) on GPU, i.e.
    # broadcasting is really done.
    log.info("synchronize with GPU: wait for broadcast() to have completed")
    ev2.synchronize()
    elapsed_s = cupy.cuda.get_elapsed_time(ev1, ev2) / 1000.0
    log.info("broadcast-0-to-%s took overall on GPU: %.3f s", follower_ranks, elapsed_s)
    log_transfer_stats(
        elapsed_s,
        f"broadcast-0->{follower_ranks}({myrank})",
        broadcast_factor=len(follower_ranks),
    )


def log_transfer_stats(elapsed_s: float, descr: str, broadcast_factor=0):
    """
    nvbandwidth measures in GB/s, i.e. actual Giga and not Gibi.
    See https://github.com/NVIDIA/nvbandwidth/blob/v0.7/memcpy.cpp#L512

    which prepares a `bandwidth` metric in Bytes/s, and then converts to
    GB/s via `(double)bandwidth * 1e-9`.
    """
    # Amount of data transferred in GB, devided by time it took to communicate
    # in seconds.
    amount_gib = SENDRECV_LOOP_REPETITIONS * (
        (LARGE_PAYLOAD_VALUE_COUNT * LARGE_PAYLOAD_BYTES_PER_VALUE)
        # / (1024 * 1024 * 1024)
        / (10**9)
    )

    if broadcast_factor != 0:
        # There is no right and wrong here. It's certainly interesting to look
        # at the total amount of data communicated in the system (in an abstract
        # sense), and use that for a bandwidth calculation.
        amount_gib = amount_gib * broadcast_factor

    log.info(
        "%s RESULT data sent: %.3f GB, time elapsed: %.3f s",
        descr,
        amount_gib,
        elapsed_s,
    )
    bandwidth_gib_per_second = amount_gib / elapsed_s

    # Note(JP): as always, bandwidth measurement is involved. On NVLink systems,
    # in particular GB200 MNNVL, this number should not be compared to "raw
    # NVLink" performance. NCCL sits on top of that and has its own overhead.
    # Also, measurement results depend on what _exactly_ is times, and on how
    # _exactly_ bandwidth is defined in the first place. Hence, what we measure
    # here is for example also not directly comparable to nvbandwidth results
    # (nvb does not use nccl, but an in-between approach involving CUDA API
    # only, i.e. what NCCL uses internally (for _some_ things). I have found
    # thaton GB200 systems indeed I get to those numbers that the NCCL team is
    # mentioning in various conversational threads. For example: "on GB200
    # allreduce at 16G message size without NVLS: 650-680 GB/s". In
    # uni-directional benchmark tests using just ncclSend() and ncclRecv() in
    # nickelpie, I regularly see numbers like 696.073 GB/s which are rather
    # consistent with that. I also have read that "NVLS gets you faster perf,
    # 830-900 GB/s" -- but that is about aggregations like ncclAllReduce().
    #
    # Other statements, ripped out of context, but providing a broad picture:
    #
    # - "Within 1 rack with 18 nodes GB200 NVL72 for alltoall why does the BW
    #   performance degrade at 14 nodes. It goes from 1 node: ~680 GB/s; 2 to 12
    #   nodes: ~666 GB/s; 14 to 18 nodes: ~630 GB/s"
    # - "Allreduce should reach over 850GB/s with NVLS around 680GB/s without.
    #   A2A should hit about 630GB/s"
    log.info("%s RESULT bandwidth: %.3f GB/s", descr, bandwidth_gib_per_second)


def generate_rnd_matrix() -> tuple[MatrixBuf, float]:
    """
    Operate on a GPU, prepare data to send. The `data` return value references
    that data (potentially a large payload residing in GPU memory).
    """
    # Generate 2D arr (matrix) for sending. msize=5 means: 10^10 values,
    # i.e. 10 billion values. Each value being four bytes (32 bit
    # float). Overall, that's about 40 GB of memory expected. nvidia-smi
    # shows, expectedly, 39232MiB.

    log.info(
        "generate rnd matrix: shape: %s, dtype: %s, (size: %.3f MB)",
        LARGE_PAYLOAD_SHAPE,
        LARGE_PAYLOAD_DTYPE,
        LARGE_PAYLOAD_SIZE_MB,
    )

    # Note(JP): I resorted to using `cast` to reliably override whatever types
    # cupy was indicating or not indicating (lots of Unknown).
    t0 = time.monotonic()
    data = cast(
        MatrixBuf, cupy.random.random(LARGE_PAYLOAD_SHAPE, dtype=LARGE_PAYLOAD_DTYPE)
    )
    t1 = time.monotonic()
    checksum = cast(float, cupy.sum(data))
    t2 = time.monotonic()

    # Note that below's timings might be flawed because this is CPU land
    # perspective. Proper timing can be done with CUDA events.
    log.info(
        "generate rnd matrix: durations: rndgen: %.3f s, sum: %.3f s",
        t1 - t0,
        t2 - t1,
    )
    log.info("generate rnd matrix: checksum: %s", checksum)

    return data, checksum


def create_nccl_communicator(n_ranks: int, comms_id, rank: int):
    with Timer("nccl.NcclCommunicator()"):
        # Note: uses ncclCommInitRank() under the hood. Expected to block until
        # the corresponding call is made by other jobs: "ncclCommInitRank
        # implicitly synchronizes with other ranks, hence it must be called by
        # different threads/processes".
        c = nccl.NcclCommunicator(ndev=n_ranks, commId=comms_id, rank=rank)  # type: ignore

        # The goal of the below code is to wait until the communicator is
        # ready. As such, "still in progress" might be an expected error. I
        # have not seen this code catch an "in progress" error yet however.
        while True:
            try:
                c.check_async_error()
                break
            except Exception as exc:
                log.info("communicator not ready, retry. Result: %s", exc)
            time.sleep(0.5)

    # log.info("wait for gpu stream to see and process post-nccl-comm-create event")
    # explicitly wait until event has been processed by cuda stream, meaning
    # that the nccl communicator work is done on-gpu
    e = cupy.cuda.Event()
    e.record()
    e.synchronize()
    log.info("post-nccl-comm-create event seen")

    return c


def tear_down_communicator(c):
    log.info("tear down communicator")

    while True:
        try:
            c.check_async_error()
            break
        except Exception as exc:
            log.info("communicator not ready, retry. Result: %s", exc)
        time.sleep(0.5)

    # NCCL communicator teardown. Docs are a little unclear. Let's see.
    log.info("nccl: comm destroy()")
    c.destroy()
    log.info("nccl: comm destroy() returned")


def sync_with_follower_on_barrier(bname: str):
    """
    Signal to the HTTP server (running in another thread) that it can tell the
    follower that we've reached this barrier, and that we're ready to move past
    it.
    """
    enames = ("BARRIER_LEADER_" + bname, "BARRIER_FOLLOWER_" + bname)
    for name in enames:
        # Set value if not already present. Atomic (thread-safe).
        EVENTS.setdefault(name, threading.Event())

    # Inform the HTTP handler that it's allowed to respond _now_.
    EVENTS["BARRIER_LEADER_" + bname].set()

    # Wait for the follower to have told us (via HTTP) that it also has reached
    # the barrier.
    log.info("sync: wait until follower reached barrier %s", bname)
    EVENTS["BARRIER_FOLLOWER_" + bname].wait()

    log.info("sync: follower reached barrier %s -- move on", bname)


def sync_with_leader_on_barrier(bname) -> None:
    """
    Implement two-way handshake between leader and follower.

    Send request to /ready-to-recv endpoint, indicating to the leader that we
    are ready to enter the recv() loop.

    Block on receiving a response from the leader.

    Once a response arrives we know that the leader is ready to enter the send
    loop.

    That allows for both of us to enter the recv()-send()-loop at approximately
    the same wall time and that makes for a useful overall measurement of
    completion time, where the total time spent in the recv() loop (recipient
    point of view) or send() loop (sender point of view) corresponds to
    approximately the total time spent in communication. That is highly relevant
    for more or less meaningful bandwidth measurement.
    """
    url = f"{LEADER_HTTPD_BASE_URL}/sync-on-" + bname

    log.info("send request to %s and wait for resp", url)
    while True:
        try:
            # Short connect() timeout, but long recv() timeout to implement sync
            # barrier.
            resp = requests.get(url, timeout=(4, 300))
            break
        except requests.exceptions.RequestException as exc:
            log.info("request failed, retry soon -- err was: %s", exc)
            time.sleep(1)

    log.info("reached leader, got resp body: %s", resp.content)
    assert resp.text == "move"


def wait_for_comm_id_from_leader() -> tuple:
    """
    Wait until HTTP server becomes reachable, and read NCCL communication ID
    from leader. Return it.
    """

    url = f"{LEADER_HTTPD_BASE_URL}/communication-id"

    log.info("enter loop: get NCCL communication ID from leader, URL: %s", url)

    attempt = 0

    while True:

        attempt += 1

        try:
            # Short connect()/recv() timeout to keep logs flowing.
            resp = requests.get(url, timeout=(4, 6))
            if resp.status_code == 200:
                break
            log.info(
                "unexpected response, retry: %s, %s",
                resp.status_code,
                resp.content[:200],
            )
        except requests.exceptions.RequestException as exc:
            # dns err, connect or recv timeout, etc
            if attempt % 10 == 0:
                log.info("request %s failed, retry soon -- err was: %s", attempt, exc)

        # retry fast, therefore don't log err outcome of every attempt
        time.sleep(0.5)

    log.info("leader, got resp body (hex prefix): %s...", resp.content[:5].hex())

    # Pickle-decode communication ID, it's originally a Python tuple.
    nccl_communication_id = pickle.loads(resp.content)
    log.info("NCCL communication ID: %s", nccl_communication_id)
    assert isinstance(nccl_communication_id, tuple)

    return nccl_communication_id


class HTTPHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        # Handles any GET (any path)

        message = b"unknown path"

        # poor-person's by-path handler:
        if "/communication-id" in self.path:
            # The ID is a tuple. For safe transfer: encode to byte string.
            comms_id_serialized = pickle.dumps(DYNCFG["nccl_comm_id"])
            log.info(
                "HTTPD: send serialized NCCL communication ID: %s...",
                comms_id_serialized[:6],
            )
            message = comms_id_serialized + b"\n"

        if "/sync-on-" in self.path:
            # The follower came here to signal us that they are ready to move
            # past a named barrier. Are we ready to move past that, however?
            # Block until we our sender thread is indicating so, and delay
            # sending the HTTP response out to the client (it has a ~long recv()
            # timout set).

            barrier_name = self.path.split("sync-on-")[1]
            log.info(
                "HTTPD: follower reached barrier %s, block emitting response until leader ready",
                barrier_name,
            )

            # Set value of not already present. Atomic (thread-safe). This means
            # a bad HTTP client can create such event objects. That's OK for
            # now. I just don't want to pre-create all events (requires a priori
            # known names of all barriers), but instead write barrier names only
            # in the call sites.
            enames = (
                "BARRIER_FOLLOWER_" + barrier_name,
                "BARRIER_LEADER_" + barrier_name,
            )
            for name in enames:
                EVENTS.setdefault(name, threading.Event())

            follower_ready = EVENTS["BARRIER_FOLLOWER_" + barrier_name]
            leader_ready = EVENTS["BARRIER_LEADER_" + barrier_name]

            # Block until sender thread sets this. Might already be unblocked.
            leader_ready.wait()

            log.info(
                "HTTPD: leader reached barrier %s. Notify follower to move past this (via HTTP response)",
                barrier_name,
            )

            # Notify sender thread that indeed now it's supposed to enter the send loop.
            follower_ready.set()
            message = b"move"

        self.send_response(200)
        self.end_headers()
        self.wfile.write(message)
        log.debug("served request to %s", self.path)
        return


def run_httpd_in_thread():
    def run():
        log.info("start HTTP server")

        # Handle each incoming request in its own thread.
        s = ThreadingHTTPServer(
            # Listen on all interfaces.
            ("0.0.0.0", int(os.environ.get("NICKELPIE_HTTPD_PORT"))),
            HTTPHandler,
        )

        # Block forever.
        s.serve_forever()

    # Start main loop of this server in its own thread.
    # Set name, used in logger.
    t = threading.Thread(target=run, name="httpd")

    # Program exits when only daemon threads are left (no explicit join
    # required in this case).
    t.daemon = True
    t.start()


class Timer:
    """
    JP's quick version of a context manager that measures duration (wall time)
    spent in the context.
    """

    def __init__(self, descr):
        # Human-friendly short description of the operation that is being timed,
        # used in log msgs.
        self.descr = descr

    def __enter__(self):
        # Keep record of wall time upon entry. While not precise, this can still
        # help correlate events in a distributed system when clocks are OK-ish
        # aligned.
        log.info("start: %s", self.descr)
        self.t0_wall = datetime.now().strftime("%H:%M:%S.%f")
        self.t0 = time.monotonic()
        return self

    def __exit__(self, _, __, ___):
        t1 = time.monotonic()
        dur = t1 - self.t0
        log.info("done: %s, took %.6f s (started at %s)", self.descr, dur, self.t0_wall)


def log_debug_info_assert_env():
    """
    Check on some invariants and log debug info.
    """
    log.info("cupy %s -- %s", cupy.__version__, cupy.__file__)
    log.info("nccl available: %s", nccl.available)
    log.info("nccl version: %s", nccl.get_version())
    log.info("cuda driverGetVersion(): %s", cupy.cuda.runtime.driverGetVersion())
    log.info(
        "cuda get_local_runtime_version: %s", cupy.cuda.get_local_runtime_version()
    )

    devcount = cupy.cuda.runtime.getDeviceCount()

    log.info("getDeviceCount(): %s", devcount)

    # Log CUDA-related environment. Extend allow-list as needed.
    # for k, v in os.environ.items():
    #     if "CUDA" in k or "NVIDIA" in k:
    #         log.info("env: %s: %s", k, v)

    try:
        log.info(
            "listdir(/dev/nvidia-caps-imex-channels): %s",
            os.listdir("/dev/nvidia-caps-imex-channels"),
        )
    except Exception as exc:
        log.info("cannot enumerate /dev/nvidia-caps-imex-channels: %s", exc)

    try:
        devs = open("/proc/devices", "rb").read().decode("utf-8").splitlines()
        log.info(
            "/proc/devices contains IMEX devices: %s",
            [d for d in devs if "imex" in d.lower()],
        )
    except Exception as exc:
        log.info("cannot inspect /proc/devices: %s", exc)

    dinfo_path = "/proc/driver/nvidia/version"
    try:
        log.info("%s: %s", dinfo_path, open(dinfo_path, "rb").read().decode("utf-8"))
    except Exception as exc:
        log.info("cannot inspect /proc/devices: %s", exc)

    # For simplicity, towards single-GPU-per-container
    assert devcount == 1, "precisely one CUDA device expected"

    # Expect to be run as part of an indexed, parallel k8s job.
    assert "JOB_COMPLETION_INDEX" in os.environ, "JOB_COMPLETION_INDEX not set in env"

    log.info("NICKELPIE_HTTPD_DNSNAME: %s", os.environ.get("NICKELPIE_HTTPD_DNSNAME"))
    log.info("NICKELPIE_HTTPD_PORT: %s", os.environ.get("NICKELPIE_HTTPD_PORT"))

    log_device_properties()


def log_device_properties():
    """
    Look up properties of that one device that has been made visible to process.
    Log some properties. Extend allow-list as needed.
    """

    _attr_filter = ["name", "pci", "uuid", "multi", "minor", "major"]

    # Assume there is just one device injected / available, and get its id
    devidx = cupy.cuda.runtime.getDevice()

    # for devidx in CUDA_DEVICE_INDICES:
    for devidx in (devidx,):
        dev = cupy.cuda.Device(devidx)
        props = cupy.cuda.runtime.getDeviceProperties(devidx)

        printprops = {}
        for k, v in props.items():
            for ss in _attr_filter:
                if k.startswith(ss):
                    if k == "uuid":
                        try:
                            v = uuid.UUID(bytes=v)
                        except ValueError:
                            # For some devices in some cases this does not seem to
                            # report proper UUID raw data:
                            # raise ValueError('bytes is not a 16-char string')
                            log.warning("funky UUID bytes: %s", v)
                            # Proceed with that funky value.
                    printprops[k] = v
                    break

        log.info("%s properties:\n%s", dev, pformat(printprops))


def sigterm_handler(_signo, _stack_frame):
    # Raise SystemExit():
    sys.exit("RECEIVED SIGTERM")


if sys.argv[1] == "handle_signal":
    signal.signal(signal.SIGTERM, sigterm_handler)


if __name__ == "__main__":
    # Set name for logging, same length as 'httpd'
    threading.current_thread().name = " main"

    # Always have exit code 0 to not have the k8s job restart pods
    try:
        main()
    except Exception:
        log.exception("main() crash:")
        log.info("exit 0")
        sys.exit(0)
