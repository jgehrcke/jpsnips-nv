apiVersion: resource.nvidia.com/v1beta1
kind: ComputeDomain
metadata:
  name: repro1-compute-domain
spec:
  numNodes: 2
  channel:
    resourceClaimTemplate:
      name: repro1-compute-domain-rct
---
apiVersion: v1
kind: Service
metadata:
  name: svc-repro1
spec:
  clusterIP: None
  selector:
    job-name: repro1
---
apiVersion: batch/v1
kind: Job
metadata:
  name: repro1
spec:
  completions: 2
  parallelism: 2
  completionMode: Indexed
  template:
    metadata:
      labels:
        jobname: repro1-worker
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: jobname
                operator: In
                values:
                - repro1-worker
            topologyKey: nvidia.com/gpu.clique
        # Make job copies land on different nodes
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: jobname
                operator: In
                values:
                - repro1-worker
            topologyKey: "kubernetes.io/hostname"
    spec:
      restartPolicy: Never
      subdomain: svc-repro1
      containers:
      - name: repro1
        image: docker.io/jgehrcke/fiberrepro:v250324
        command: ["python", "/thing/fabric-handle-transfer-test.py"]
        env:
        - name: LEADER_HTTPD_DNSNAME
          value: "repro1-0.svc-repro1"
        - name: LEADER_HTTPD_PORT
          value: "1337"
        resources:
          limits:
            nvidia.com/gpu: 1
          claims:
          - name: shared-imex-channel
      resourceClaims:
      - name: shared-imex-channel
        resourceClaimTemplateName: repro1-compute-domain-rct

